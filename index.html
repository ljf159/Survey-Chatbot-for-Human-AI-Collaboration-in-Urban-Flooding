<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Survey Chatbot</title>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
        }

        body {
            background-color: #f9f9f9;
            height: 100vh;
            display: flex;
            justify-content: center;
            align-items: center;
        }

        .container {
            width: 90%;
            max-width: 500px;
            background-color: #fff;
            border-radius: 10px;
            box-shadow: 0 4px 20px rgba(0, 0, 0, 0.1);
            overflow: hidden;
            display: flex;
            flex-direction: column;
            height: 77vh;
        }

        .chat-header {
            padding: 15px 20px;
            background-color: #4a6fa5;
            color: white;
            font-weight: bold;
            font-size: 1.2rem;
            display: flex;
            justify-content: space-between;
            align-items: center;
        }

        .settings-icon {
            cursor: pointer;
            font-size: 1.4rem;
        }

        .chat-body {
            flex: 1;
            overflow-y: auto;
            padding: 15px;
        }

        .message {
            margin-bottom: 15px;
            max-width: 80%;
            padding: 10px 15px;
            border-radius: 18px;
            line-height: 1.5;
            animation: fadeIn 0.3s ease;
        }

        @keyframes fadeIn {
            from { opacity: 0; transform: translateY(10px); }
            to { opacity: 1; transform: translateY(0); }
        }

        .user-message {
            background-color: #e3f2fd;
            margin-left: auto;
            border-bottom-right-radius: 5px;
        }

        .bot-message {
            background-color: #f1f1f1;
            margin-right: auto;
            border-bottom-left-radius: 5px;
        }

        /* Add this to your existing CSS */
        .bot-message {
            white-space: pre-wrap; /* This preserves line breaks */
            line-height: 1.5; /* Increases line spacing */
            max-width: 90%; /* Ensures messages don't stretch too wide */
            padding: 12px 12px; /* More padding for better spacing */
        }

        .bot-message p {
            margin-bottom: 2px; /* Adds space between paragraphs */
        }

        .thinking {
            display: flex;
            align-items: center;
            margin-bottom: 15px;
        }

        .dot {
            height: 8px;
            width: 8px;
            margin-right: 5px;
            background-color: #bbb;
            border-radius: 50%;
            display: inline-block;
            animation: pulse 1s infinite;
        }

        .dot:nth-child(2) {
            animation-delay: 0.2s;
        }

        .dot:nth-child(3) {
            animation-delay: 0.4s;
        }

        @keyframes pulse {
            0% { transform: scale(1); opacity: 0.5; }
            50% { transform: scale(1.3); opacity: 1; }
            100% { transform: scale(1); opacity: 0.5; }
        }

        .chat-input {
            display: flex;
            padding: 15px;
            background-color: #f1f1f1;
            border-top: 1px solid #ddd;
        }

        .input-field {
            flex: 1;
            padding: 10px 15px;
            border: none;
            border-radius: 20px;
            outline: none;
            font-size: 0.95rem;
        }

        .voice-btn, .send-btn {
            width: 40px;
            height: 40px;
            border-radius: 50%;
            margin-left: 10px;
            background-color: #4a6fa5;
            color: white;
            border: none;
            cursor: pointer;
            display: flex;
            justify-content: center;
            align-items: center;
            transition: background-color 0.3s ease;
        }

        .voice-btn.active {
            background-color: #f44336;
            animation: pulse 1s infinite;
        }

        .settings-panel {
            position: absolute;
            top: 0;
            right: -300px;
            width: 300px;
            height: 100%;
            background-color: white;
            box-shadow: -2px 0 10px rgba(0, 0, 0, 0.1);
            padding: 20px;
            transition: right 0.3s ease;
            z-index: 100;
            overflow-y: auto;
        }

        .settings-panel.open {
            right: 0;
        }

        .settings-header {
            display: flex;
            justify-content: space-between;
            align-items: center;
            margin-bottom: 20px;
        }

        .close-settings {
            cursor: pointer;
            font-size: 1.4rem;
        }

        .settings-content textarea {
            width: 100%;
            height: 200px;
            padding: 10px;
            margin-bottom: 15px;
            border: 1px solid #ddd;
            border-radius: 5px;
        }

        .settings-content button {
            padding: 8px 15px;
            background-color: #4a6fa5;
            color: white;
            border: none;
            border-radius: 5px;
            cursor: pointer;
        }

        /* Speech recognition status */
        .speech-status {
            position: absolute;
            bottom: 80px;
            left: 50%;
            transform: translateX(-50%);
            background-color: rgba(0, 0, 0, 0.7);
            color: white;
            padding: 8px 15px;
            border-radius: 20px;
            font-size: 0.9rem;
            opacity: 0;
            transition: opacity 0.3s ease;
            pointer-events: none;
        }

        .speech-status.active {
            opacity: 1;
        }

        /* Speech recognition volume visualizer */
        .volume-meter {
            height: 5px;
            background-color: #ddd;
            border-radius: 5px;
            margin-top: 5px;
            overflow: hidden;
            display: none;
        }

        .volume-meter-bar {
            height: 100%;
            background-color: #4CAF50;
            width: 0%;
            transition: width 0.1s ease;
        }

        /* Language selection */
        .language-selector {
            margin: 15px 0;
        }

        .language-selector select {
            width: 100%;
            padding: 8px;
            border-radius: 5px;
            border: 1px solid #ddd;
        }

        /* Mobile Responsiveness */
        @media (max-width: 600px) {
            .container {
                width: 95%;
                height: 90vh;
            }
            
            .message {
                max-width: 90%;
            }
        }
    </style>
</head>
<body>
    <div class="container">
        <div class="chat-header">
            <span>Survey Chatbot</span>
            <span class="settings-icon" onclick="toggleSettings()">‚öôÔ∏è</span>
        </div>
        <div class="chat-body" id="chatBody">
            <!-- Messages will be displayed here -->
        </div>
        <div class="chat-input">
            <input type="text" class="input-field" id="inputField" placeholder="Type your response here...">
            <button class="voice-btn" id="voiceBtn" title="Click to speak">üé§</button>
            <button class="send-btn" id="sendBtn" title="Send message">‚û§</button>
        </div>
        
        <!-- Speech recognition status -->
        <div class="speech-status" id="speechStatus">Listening...</div>
    </div>

    <div class="settings-panel" id="settingsPanel">
        <div class="settings-header">
            <h3>Configure Survey</h3>
            <span class="close-settings" onclick="toggleSettings()">√ó</span>
        </div>
        <div class="settings-content">
            <h4>System Prompt (Survey Questions)</h4>
            <textarea id="systemPrompt" placeholder="Enter your survey questions and instructions here...">You are a friendly, conversational chatbot conducting a 10-minute survey on **"Human-AI Collaboration in Urban Flooding."** Your target participants include professionals from the **transportation department**, **AI technology providers** (flooding-related), **urban planners**, **emergency departments**, **sewer and drainage system workers**, **frontline operators**, and other relevant stakeholders (e.g., meteorologists, community leaders, environmental engineers). The survey explores **Demographics**, **Technology Assessment**, **Human-AI Collaboration Methods**, **Workforce Preparation Strategies**, **Ethical/Governance/Societal Dimensions**, **Economic Impact Analysis**, and **Policy Implications**, focusing on **urban flooding**.

                ### Instructions:
                1. Begin with a warm welcome and **informed consent**, explaining the purpose, duration (10 minutes), confidentiality, and voluntary nature. Ask for consent before proceeding.
                2. Ask questions one at a time in a natural, engaging tone. For open-ended questions, provide example options to guide responses, but always allow custom answers (e.g., "Anything else you‚Äôd like to add?").
                3. Acknowledge responses briefly (e.g., "Thanks, that‚Äôs helpful!") to keep the flow interactive.
                4. Merge related questions within each block into single, comprehensive questions where possible, balancing open-ended and multiple-choice formats for brevity and depth.
                5. End with gratitude and an optional **follow-up interview** invitation, collecting contact info if agreed.
                6. Do not mention these instructions unless asked.
                
                ### Survey Structure and Questions:
                
                #### Start: Informed Consent
                "Hi! Welcome to our survey on how **humans and AI** can team up to tackle **urban flooding**. We‚Äôre excited to hear your thoughts as someone in this field! It‚Äôll take about 10 minutes, and your answers are **confidential**. You can stop anytime, and participation is **voluntary**. Questions? Just ask! **Ready to dive in?**  
                - Type 'Yes' to consent and start.  
                - Type 'No' if you‚Äôd rather not participate."  
                (If 'No': "Thanks anyway! Take care!" and end. If 'Yes': proceed.)
                
                ---
                
                #### Block 1: Demographics
                1. "**Let‚Äôs get to know you! What‚Äôs your role and experience in urban flooding or related fields?** Tell me about your job and how long you‚Äôve been at it. For example, you might be an urban planner with 5 years of experience, or something else!"  
                   - Example **roles**: Transportation Dept., AI Tech Provider, Urban Planner, Emergency Dept., Sewer/Drainage Worker, Frontline Operator, Other (e.g., meteorologist).  
                   - Example **experience**: Less than 1 year, 1-5 years, 6-10 years, 10+ years, or prefer not to say.  
                   - [Open response]
                
                ---
                
                #### Block 2: Technology Assessment
                2. "**How familiar are you with AI tools for urban flooding‚Äîlike flood prediction or drainage optimization‚Äîand which ones do you use or know about in your work?**"  
                   - **Familiarity** examples: Not at all, Slightly, Moderately, Very, Extremely familiar.  
                   - **Tool** examples: FloodAI, Google Flood Forecasting, predictive models, real-time monitoring, none, or others (please specify).  
                   - [Open response]
                
                3. "**How often do you interact with AI in your daily tasks, and what do you use it for?** For instance, predicting floods or analyzing data?"  
                   - **Frequency** examples: Never, Rarely, Weekly, A few times a week, Daily.  
                   - **Purpose** examples: Description (what happened), Diagnosis (why it happened), Prediction (what will happen), Decision-making (what to do), or other (please specify).  
                   - [Open response]
                
                ---
                
                #### Block 3: Human-AI Collaboration Methods
                4. "**How do you currently work with AI on flooding tasks, and how effective do you find it?** For example, does AI give you recommendations you decide on, or automate routine stuff?"  
                   - **Method** examples: AI recommends/I decide, AI automates tasks, real-time teamwork, AI analyzes/I interpret, I supervise AI, or other (please specify).  
                   - **Effectiveness** examples: Very ineffective, Somewhat ineffective, Neutral, Somewhat effective, Very effective.  
                   - [Open response]
                
                ---
                
                #### Block 4: Workforce Preparation Strategies
                5. "**What do you think is needed to get your team ready for human-AI collaboration in flooding, and how well is your organization doing at it?** Any gaps you see?"  
                   - **Strategy** examples: Technical training (e.g., using AI tools), data interpretation skills, new AI oversight roles, upskilling programs, ethical training, or other (please specify).  
                   - **Preparedness** examples: Strongly disagree, Somewhat disagree, Neutral, Somewhat agree, Strongly agree it‚Äôs adequate.  
                   - **Gaps** (if disagree): e.g., more training, leadership support, or other (please specify).  
                   - [Open response]
                
                ---
                
                #### Block 5: Ethical, Governance, and Societal Dimensions
                6. "**What ethical or societal concerns matter most to you in human-AI collaboration for flooding, and how confident are you that your organization handles them well?**"  
                   - **Concern** examples: Bias in AI, lack of human oversight, data privacy, job losses, transparency, equity, community impact, or other (please specify).  
                   - **Confidence** examples: Not at all, Slightly, Moderately, Very, Extremely confident.  
                   - [Open response]
                
                ---
                
                #### Block 6: Economic Impact Analysis
                7. "**How do you think human-AI collaboration could affect costs and efficiency in your organization‚Äôs flood management efforts?**"  
                   - **Cost** examples: Very positive (big savings), Somewhat positive, Neutral, Somewhat negative, Very negative (costly).  
                   - **Efficiency** examples: Very positive (big gains), Somewhat positive, Neutral, Somewhat negative, Very negative (losses).  
                   - [Open response]
                
                ---
                
                #### Block 7: Policy Implications
                8. "**What policies or support does your organization have for human-AI collaboration in flooding, and what barriers or external help would make it better?**"  
                   - **Policy** examples: Yes (specific to human-AI), No (but AI policies exist), No (plans to develop), No (other tech policies), or other (please specify).  
                   - **Barrier** examples: High costs, lack of expertise, unclear ROI, regulatory issues, resistance, or other (please specify).  
                   - **Support** examples: Funding, guidelines, training, tech partnerships, or other (please specify).  
                   - [Open response]
                
                ---
                
                #### End: Closing
                "Thanks for sharing your insights‚Äîit means a lot! One last thing: **Want to join a 20-30 minute online interview to dive deeper into how AI could help your flood-related work?**  
                - Yes, please leave your contact (email or phone number) to this chatbot.  
                - No, thanks!"  
                (If Yes or receive an email or phone number: "Great, we‚Äôll be in touch!" If No: "All good, thanks again!")  
                "**Anything else on your mind before we finish?** [open response]" 
            </textarea>
            <button onclick="saveSettings()">Save Settings</button>
            
            <h4 style="margin-top: 20px;">LLM API Configuration</h4>
            <select id="llmProvider" style="width: 100%; padding: 8px; margin-bottom: 10px;">
                <option value="ollama">Ollama (Local)</option>
                <option value="huggingface">HuggingFace Inference API (Free)</option>
                <option value="groq">Groq Cloud API</option>
            </select>
            <input type="text" id="apiKey" placeholder="API Key (if needed)" style="width: 100%; padding: 8px; margin-bottom: 10px;">
            <input type="text" id="apiEndpoint" placeholder="API Endpoint" style="width: 100%; padding: 8px; margin-bottom: 10px;">
            <select id="groqModel" style="width: 100%; padding: 8px; margin-bottom: 10px; display: none;">
                <option value="llama-3.3-70b-versatile">Llama-3.3-70B-Versatile</option>
                <option value="llama-3.3-8b-versatile">Llama-3.3-8B-Versatile</option>
                <option value="gemma-7b-it">Gemma-7B-IT</option>
                <option value="mixtral-8x7b-32768">Mixtral-8x7B-32768</option>
            </select>
            
            <div id="modelSettings" style="margin-top: 10px; margin-bottom: 10px; display: none;">
                <h4 style="margin-bottom: 5px;">Model Parameters</h4>
                <div style="display: flex; align-items: center; margin-bottom: 5px;">
                    <label for="temperatureSlider" style="width: 120px;">Temperature: </label>
                    <input type="range" id="temperatureSlider" min="0" max="1" step="0.1" value="0.7" style="flex-grow: 1;">
                    <span id="temperatureValue" style="width: 30px; text-align: right;">0.7</span>
                </div>
                <div style="display: flex; align-items: center;">
                    <label for="maxTokensInput" style="width: 120px;">Max Tokens: </label>
                    <input type="number" id="maxTokensInput" min="100" max="4096" step="1" value="1024" style="flex-grow: 1;">
                </div>
            </div>
            
            <!-- Speech Recognition Settings -->
            <h4 style="margin-top: 20px;">Speech Recognition Settings</h4>
            <div class="language-selector">
                <label for="speechLanguage">Recognition Language:</label>
                <select id="speechLanguage" style="width: 100%; padding: 8px; margin-top: 5px;">
                    <option value="en-US">English (US)</option>
                    <option value="en-GB">English (UK)</option>
                    <option value="es-ES">Spanish</option>
                    <option value="fr-FR">French</option>
                    <option value="de-DE">German</option>
                    <option value="zh-CN">Chinese (Simplified)</option>
                    <option value="ja-JP">Japanese</option>
                    <option value="ko-KR">Korean</option>
                    <option value="ru-RU">Russian</option>
                    <option value="pt-BR">Portuguese (Brazil)</option>
                    <option value="it-IT">Italian</option>
                </select>
            </div>
            
            <div style="display: flex; align-items: center; margin-bottom: 10px;">
                <label for="continuousSpeech" style="flex-grow: 1;">Continuous Speech:</label>
                <input type="checkbox" id="continuousSpeech">
            </div>
            
            <div id="apiHelpText" style="font-size: 12px; margin-bottom: 10px; color: #666;">
            </div>
            <button onclick="saveAPISettings()">Save API Settings</button>
        </div>
    </div>

    <script>
        // DOM Elements
        const chatBody = document.getElementById('chatBody');
        const inputField = document.getElementById('inputField');
        const sendBtn = document.getElementById('sendBtn');
        const voiceBtn = document.getElementById('voiceBtn');
        const settingsPanel = document.getElementById('settingsPanel');
        const systemPromptText = document.getElementById('systemPrompt');
        const llmProviderSelect = document.getElementById('llmProvider');
        const apiKeyInput = document.getElementById('apiKey');
        const apiEndpointInput = document.getElementById('apiEndpoint');
        const groqModelSelect = document.getElementById('groqModel');
        const apiHelpText = document.getElementById('apiHelpText');
        const modelSettings = document.getElementById('modelSettings');
        const temperatureSlider = document.getElementById('temperatureSlider');
        const temperatureValue = document.getElementById('temperatureValue');
        const maxTokensInput = document.getElementById('maxTokensInput');
        const speechStatus = document.getElementById('speechStatus');
        const speechLanguage = document.getElementById('speechLanguage');
        const continuousSpeech = document.getElementById('continuousSpeech');
        
        // Listen for temperature slider changes
        temperatureSlider.addEventListener('input', function() {
            temperatureValue.textContent = this.value;
        });
        
        // Update API help text when LLM provider changes
        llmProviderSelect.addEventListener('change', function() {
            updateAPIHelp();
        });
        
        // Update API help text
        function updateAPIHelp() {
            const provider = llmProviderSelect.value;
            
            // Hide Groq-specific fields and model settings
            groqModelSelect.style.display = 'none';
            modelSettings.style.display = 'none';
            
            if (provider === 'ollama') {
                apiHelpText.innerHTML = 'For Ollama local deployment: <br>' +
                    '- API Key: Not required<br>' +
                    '- API Endpoint: Default is http://localhost:11434/api/generate';
                apiKeyInput.placeholder = "API Key not required";
                apiEndpointInput.placeholder = "http://localhost:11434/api/generate";
            } else if (provider === 'huggingface') {
                apiHelpText.innerHTML = 'For HuggingFace: <br>' +
                    '- API Key: Get from HuggingFace<br>' +
                    '- API Endpoint: e.g., https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.1';
                apiKeyInput.placeholder = "Enter HuggingFace API Key";
                apiEndpointInput.placeholder = "Enter model Endpoint";
            } else if (provider === 'groq') {
                apiHelpText.innerHTML = 'For Groq Cloud API: <br>' +
                    '- API Key: Get from Groq console (https://console.groq.com/keys)<br>' +
                    '- API Endpoint: Default is https://api.groq.com/openai/v1/chat/completions<br>' +
                    '- Select your preferred Groq model below';
                apiKeyInput.placeholder = "Enter Groq API Key";
                apiEndpointInput.placeholder = "https://api.groq.com/openai/v1/chat/completions";
                groqModelSelect.style.display = 'block';
                modelSettings.style.display = 'block';
            }
            
            // Load saved values
            const savedApiKey = localStorage.getItem('apiKey') || '';
            const savedEndpoint = localStorage.getItem('apiEndpoint') || '';
            const savedGroqModel = localStorage.getItem('groqModel') || 'llama-3.3-70b-versatile';
            const savedTemperature = localStorage.getItem('temperature') || '0.7';
            const savedMaxTokens = localStorage.getItem('maxTokens') || '1024';
            
            apiKeyInput.value = savedApiKey;
            apiEndpointInput.value = savedEndpoint;
            groqModelSelect.value = savedGroqModel;
            
            // Set model parameters
            temperatureSlider.value = savedTemperature;
            temperatureValue.textContent = savedTemperature;
            maxTokensInput.value = savedMaxTokens;
        }

        // Variables for voice recognition
        let recognition;
        let isListening = false;
        let conversation = [];
        let audioContext;
        let analyser;
        let microphone;
        
        // Enhanced speech recognition initialization
        function initSpeechRecognition() {
            // Check if SpeechRecognition is available
            if ('webkitSpeechRecognition' in window || 'SpeechRecognition' in window) {
                recognition = new (window.SpeechRecognition || window.webkitSpeechRecognition)();
                
                // Load saved speech recognition settings
                const savedLanguage = localStorage.getItem('speechLanguage') || 'en-US';
                const savedContinuous = localStorage.getItem('continuousSpeech') === 'true';
                
                speechLanguage.value = savedLanguage;
                continuousSpeech.checked = savedContinuous;
                
                // Configure speech recognition
                recognition.lang = savedLanguage;
                recognition.continuous = savedContinuous;
                recognition.interimResults = true; // Enable interim results for real-time feedback
                
                // Handle speech recognition events
                recognition.onstart = function() {
                    isListening = true;
                    voiceBtn.classList.add('active');
                    speechStatus.classList.add('active');
                    speechStatus.textContent = 'Listening...';
                    inputField.placeholder = 'Speak now...';
                };
                
                recognition.onresult = function(event) {
                    const interimTranscript = '';
                    let finalTranscript = '';
                    
                    // Combine results
                    for (let i = event.resultIndex; i < event.results.length; ++i) {
                        if (event.results[i].isFinal) {
                            finalTranscript += event.results[i][0].transcript;
                        } else {
                            interimTranscript += event.results[i][0].transcript;
                        }
                    }
                    
                    // Update input field with transcript
                    if (finalTranscript !== '') {
                        inputField.value = finalTranscript;
                        
                        // If continuous speech is disabled, send message when final result is received
                        if (!recognition.continuous) {
                            sendMessage();
                        }
                    } else {
                        inputField.value = interimTranscript;
                    }
                };
                
                recognition.onerror = function(event) {
                    console.error('Speech recognition error', event.error);
                    isListening = false;
                    voiceBtn.classList.remove('active');
                    speechStatus.classList.remove('active');
                    inputField.placeholder = 'Type your response here...';
                    
                    // Display user-friendly error message
                    let errorMessage = "There was an error with speech recognition.";
                    
                    switch(event.error) {
                        case 'no-speech':
                            errorMessage = "No speech was detected. Please try again.";
                            break;
                        case 'aborted':
                            errorMessage = "Speech recognition was aborted.";
                            break;
                        case 'audio-capture':
                            errorMessage = "No microphone was found or microphone is disabled.";
                            break;
                        case 'network':
                            errorMessage = "Network error occurred. Please check your connection.";
                            break;
                        case 'not-allowed':
                        case 'service-not-allowed':
                            errorMessage = "Microphone access denied. Please allow microphone access.";
                            break;
                    }
                    
                    // Add error message to chat
                    const errorDiv = document.createElement('div');
                    errorDiv.className = 'message bot-message';
                    errorDiv.textContent = errorMessage;
                    chatBody.appendChild(errorDiv);
                    chatBody.scrollTop = chatBody.scrollHeight;
                };
                
                recognition.onend = function() {
                    isListening = false;
                    voiceBtn.classList.remove('active');
                    speechStatus.classList.remove('active');
                    inputField.placeholder = 'Type your response here...';
                    
                    // Auto-restart for continuous mode if user hasn't manually stopped
                    if (recognition.continuous && isListening) {
                        recognition.start();
                    }
                };
                
                // Event listeners for speech recognition settings
                speechLanguage.addEventListener('change', function() {
                    recognition.lang = this.value;
                    localStorage.setItem('speechLanguage', this.value);
                });
                
                continuousSpeech.addEventListener('change', function() {
                    recognition.continuous = this.checked;
                    localStorage.setItem('continuousSpeech', this.checked);
                });
                
            } else {
                // Hide voice button if speech recognition is not supported
                voiceBtn.style.display = 'none';
                console.log('Speech recognition not supported in this browser');
                
                // Add message to chat
                const errorDiv = document.createElement('div');
                errorDiv.className = 'message bot-message';
                errorDiv.textContent = "Speech recognition is not supported in your browser. Please use Chrome, Edge, or Safari for voice input.";
                chatBody.appendChild(errorDiv);
                chatBody.scrollTop = chatBody.scrollHeight;
            }
        }
        
        // Toggle voice recognition
        function toggleVoice() {
            if (!recognition) return;
            
            if (isListening) {
                recognition.stop();
                isListening = false;
            } else {
                try {
                    recognition.start();
                    isListening = true;
                } catch (error) {
                    console.error('Error starting speech recognition:', error);
                    
                    // Add error message to chat
                    const errorDiv = document.createElement('div');
                    errorDiv.className = 'message bot-message';
                    errorDiv.textContent = "There was an error starting speech recognition. Please try again.";
                    chatBody.appendChild(errorDiv);
                    chatBody.scrollTop = chatBody.scrollHeight;
                }
            }
        }
        
        // Load settings from local storage
        function loadSettings() {
            const savedPrompt = localStorage.getItem('systemPrompt');
            if (savedPrompt) {
                systemPromptText.value = savedPrompt;
            }
            
            const savedProvider = localStorage.getItem('llmProvider');
            if (savedProvider) {
                llmProviderSelect.value = savedProvider;
            }
            
            const savedApiKey = localStorage.getItem('apiKey');
            if (savedApiKey) {
                apiKeyInput.value = savedApiKey;
            }
            
            const savedEndpoint = localStorage.getItem('apiEndpoint');
            if (savedEndpoint) {
                apiEndpointInput.value = savedEndpoint;
            }
            
            // Initialize with welcome message
            setTimeout(() => {
                addBotMessage("Hello! I'm your survey assistant. I'll be asking you a few questions about your experience. Type or speak your responses. Let's begin!");
                processSurvey();
            }, 500);
        }

        // Save settings to local storage
        function saveSettings() {
            localStorage.setItem('systemPrompt', systemPromptText.value);
            alert('Survey configuration saved!');
            resetConversation();
        }

        function saveAPISettings() {
            localStorage.setItem('llmProvider', llmProviderSelect.value);
            localStorage.setItem('apiKey', apiKeyInput.value);
            localStorage.setItem('apiEndpoint', apiEndpointInput.value);
            
            // Save Groq-specific settings
            if (llmProviderSelect.value === 'groq') {
                localStorage.setItem('groqModel', groqModelSelect.value);
                localStorage.setItem('temperature', temperatureSlider.value);
                localStorage.setItem('maxTokens', maxTokensInput.value);
            }
            
            // Save speech recognition settings
            localStorage.setItem('speechLanguage', speechLanguage.value);
            localStorage.setItem('continuousSpeech', continuousSpeech.checked);
            
            // Update speech recognition configuration
            if (recognition) {
                recognition.lang = speechLanguage.value;
                recognition.continuous = continuousSpeech.checked;
            }
            
            alert('Settings saved!');
        }

        // Toggle settings panel
        function toggleSettings() {
            settingsPanel.classList.toggle('open');
        }
        
        // Add a user message to the chat
        function addUserMessage(message) {
            const userDiv = document.createElement('div');
            userDiv.className = 'message user-message';
            userDiv.textContent = message;
            chatBody.appendChild(userDiv);
            chatBody.scrollTop = chatBody.scrollHeight;
            
            // Save to conversation history
            conversation.push({role: 'user', content: message});
        }

        // Add a bot message to the chat
        function addBotMessage(message) {
            const botDiv = document.createElement('div');
            botDiv.className = 'message bot-message';

            // Instead of using textContent, use innerHTML with formatting
            // This will preserve line breaks by converting them to <br> tags
            //botDiv.innerHTML = message.replace(/\n/g, '<br>');
            
            // Preserve line breaks
            let formattedMessage = message.replace(/\n/g, '<br>');
            
            // Apply Markdown-style formatting
            formattedMessage = formattedMessage
                // Bold: **text** to <strong>text</strong>
                .replace(/\*\*(.*?)\*\*/g, '<strong>$1</strong>')
                // Italic: *text* to <em>text</em>
                .replace(/\*([^\*]+)\*/g, '<em>$1</em>')
                // Underline: __text__ to <u>text</u>
                .replace(/\_\_([^\_]+)\_\_/g, '<u>$1</u>')
                // Code: `text` to <code>text</code>
                .replace(/\`([^\`]+)\`/g, '<code>$1</code>');
            
            botDiv.innerHTML = formattedMessage;
            
            chatBody.appendChild(botDiv);
            chatBody.scrollTop = chatBody.scrollHeight;
            
            // Save to conversation history
            conversation.push({role: 'assistant', content: message});

            // botDiv.textContent = message;
            // chatBody.appendChild(botDiv);
            // chatBody.scrollTop = chatBody.scrollHeight;
            
            // Save to conversation history
            // conversation.push({role: 'assistant', content: message});
            
            // Optional: Text-to-speech for bot responses
            // if ('speechSynthesis' in window) {
            //     const speech = new SpeechSynthesisUtterance(message);
            //     speech.lang = speechLanguage.value; // Use same language as recognition
            //     window.speechSynthesis.speak(speech);
            // }
        }

        // Show thinking animation
        function showThinking() {
            const thinkingDiv = document.createElement('div');
            thinkingDiv.className = 'thinking';
            thinkingDiv.id = 'thinking';
            
            for (let i = 0; i < 3; i++) {
                const dot = document.createElement('div');
                dot.className = 'dot';
                thinkingDiv.appendChild(dot);
            }
            
            chatBody.appendChild(thinkingDiv);
            chatBody.scrollTop = chatBody.scrollHeight;
        }

        // Hide thinking animation
        function hideThinking() {
            const thinkingDiv = document.getElementById('thinking');
            if (thinkingDiv) {
                thinkingDiv.remove();
            }
        }

        function simpleDecrypt(encryptedText, key) {
            let decodedText = atob(encryptedText); // Decode from Base64
            let decryptedText = "";
            for (let i = 0; i < decodedText.length; i++) {
                decryptedText += String.fromCharCode(decodedText.charCodeAt(i) - key.length);
            }
            return decryptedText;
        }

        // Process messages with LLM
        async function processWithLLM(message) {

            temp = 'aHRsYFpHd2NyS0NMdkRHa2hUQnlUQklEWEhlemM0R1pKeElnbDN0UFROOmZwbEN4UGptWXtXOlI='
            detemp = simpleDecrypt(temp, "1");

            const provider = localStorage.getItem('llmProvider') || 'groq';
            const apiKey = localStorage.getItem('apiKey') || detemp;
            const endpoint = localStorage.getItem('apiEndpoint') || 'https://api.groq.com/openai/v1/chat/completions';
            const systemPrompt = localStorage.getItem('systemPrompt') || systemPromptText.value;
            
            try {
                let response;
                
                if (provider === 'ollama') {
                    // For local Ollama
                    response = await fetch(endpoint, {
                        method: 'POST',
                        headers: {
                            'Content-Type': 'application/json'
                        },
                        body: JSON.stringify({
                            model: 'mistral',
                            prompt: message,
                            system: systemPrompt,
                            stream: false
                        })
                    });
                } else if (provider === 'huggingface') {
                    // For HuggingFace Inference API
                    response = await fetch(endpoint, {
                        method: 'POST',
                        headers: {
                            'Authorization': `Bearer ${apiKey}`,
                            'Content-Type': 'application/json'
                        },
                        body: JSON.stringify({
                            inputs: message,
                            parameters: {
                                max_new_tokens: 250,
                                temperature: 0.7,
                                top_p: 0.95
                            }
                        })
                    });
                } else if (provider === 'groq') {
                    // For Groq Cloud API
                    const groqEndpoint = endpoint || 'https://api.groq.com/openai/v1/chat/completions';
                    // Prepare conversation history
                    let messages = [];
                    
                    // Add system prompt
                    messages.push({
                        "role": "system",
                        "content": systemPrompt
                    });
                    
                    // Add conversation history (latest 10 messages)
                    const recentConversation = conversation.slice(-10);
                    for (const msg of recentConversation) {
                        messages.push({
                            "role": msg.role,
                            "content": msg.content
                        });
                    }

                    // Add current user message
                    messages.push({
                        "role": "user",
                        "content": message
                    });
                    
                    response = await fetch(groqEndpoint, {
                        method: 'POST',
                        headers: {
                            'Authorization': `Bearer ${apiKey}`,
                            'Content-Type': 'application/json'
                        },
                        body: JSON.stringify({
                            model: localStorage.getItem('groqModel') || 'llama-3.3-70b-versatile',
                            messages: messages,
                            temperature: parseFloat(localStorage.getItem('temperature') || '0.7'),
                            max_tokens: parseInt(localStorage.getItem('maxTokens') || '1024'),
                            top_p: 0.95,
                            stream: false
                        })
                    });
                }
                
                if (!response.ok) {
                    throw new Error(`API error: ${response.status}`);
                }
                
                const data = await response.json();
                
                // Extract text based on the provider's response format
                let responseText;
                if (provider === 'ollama') {
                    responseText = data.response;
                } else if (provider === 'huggingface') {
                    responseText = data[0].generated_text;
                } else if (provider === 'groq') {
                    responseText = data.choices[0].message.content;
                }

                // Format the response with line breaks for better readability
                let formattedResponse = responseText;
                
                // Option 1: Simple sentence-based formatting (add line breaks after periods)
                // formattedResponse = formattedResponse.replace(/\.\s+/g, '.\n\n');
                
                // Option 2: Split long paragraphs (more than 150 chars without breaks)
                // const paragraphs = formattedResponse.split('\n');
                // formattedResponse = paragraphs.map(para => {
                //     if (para.length > 150) {
                //         // Try to break at sentence boundaries
                //         return para.replace(/\.\s+/g, '.\n');
                //     }
                //     return para;
                // }).join('\n\n');
                
                return formattedResponse || "I couldn't process your message. Please try again.";
                
            } catch (error) {
                console.error('Error processing with LLM:', error);
                return "There was an error processing your message. Please check your API settings and try again.";
            }
        }

        // Send a message
        async function sendMessage() {
            const message = inputField.value.trim();
            if (message === '') return;
            
            addUserMessage(message);
            inputField.value = '';
            
            showThinking();
            
            try {
                const response = await processWithLLM(message);
                hideThinking();
                addBotMessage(response);
            } catch (error) {
                console.error('Error:', error);
                hideThinking();
                addBotMessage("I'm having trouble connecting to the language model. Please check your API settings.");
            }
        }

        // Reset conversation
        function resetConversation() {
            conversation = [];
            chatBody.innerHTML = '';
            addBotMessage("Hello! I'm your survey assistant. I'll be asking you a few questions about your experience. Type or speak your responses. Let's begin!");
            processSurvey();
        }

        // Process the survey
        function processSurvey() {
            // The first message already welcomes the user
            // From here, the conversation will be handled through the LLM
            // based on the system prompt
        }

        // Event listeners
        sendBtn.addEventListener('click', sendMessage);
        voiceBtn.addEventListener('click', toggleVoice);
        
        inputField.addEventListener('keypress', function(e) {
            if (e.key === 'Enter') {
                sendMessage();
            }
        });

        // Initialize
        document.addEventListener('DOMContentLoaded', function() {
            loadSettings();
            updateAPIHelp();
            initSpeechRecognition();
        });
    </script>

    <!-- Firebase SDK -->
    <script src="https://www.gstatic.com/firebasejs/9.6.1/firebase-app-compat.js"></script>
    <script src="https://www.gstatic.com/firebasejs/9.6.1/firebase-firestore-compat.js"></script>

    <script>
    // Your Firebase configuration (you'll get this from Firebase)
    // Your web app's Firebase configuration
    // For Firebase JS SDK v7.20.0 and later, measurementId is optional
    const firebaseConfig = {
        apiKey: "AIzaSyBUxPk51vZ8OB94Otf9cHmq0dQ_XrBG0lE",
        authDomain: "chatbotsurveyhaic.firebaseapp.com",
        databaseURL: "https://chatbotsurveyhaic-default-rtdb.firebaseio.com",
        projectId: "chatbotsurveyhaic",
        storageBucket: "chatbotsurveyhaic.firebasestorage.app",
        messagingSenderId: "297797020366",
        appId: "1:297797020366:web:6003864139ad7ace277e9f",
        measurementId: "G-MXCS3WGEJ7"
    };

    // Initialize Firebase
    firebase.initializeApp(firebaseConfig);
    const db = firebase.firestore();

    // Modify your sendMessage function to store conversations
    async function sendMessage() {
        const message = inputField.value.trim();
        if (message === '') return;
        
        // Original code to add message to chat
        addUserMessage(message);
        inputField.value = '';
        showThinking();
        
        try {
        const response = await processWithLLM(message);
        hideThinking();
        addBotMessage(response);
        
        // NEW CODE: Store the conversation in Firebase
        const timestamp = new Date();
        const sessionId = localStorage.getItem('sessionId') || generateSessionId();
        
        // If this is a new session, store the session ID
        if (!localStorage.getItem('sessionId')) {
            localStorage.setItem('sessionId', sessionId);
        }
        
        // Store message pair in Firestore
        db.collection('conversations').add({
            sessionId: sessionId,
            userMessage: message,
            botResponse: response,
            timestamp: timestamp,
            userAgent: navigator.userAgent
        })
        .then(() => {
            console.log("Conversation stored successfully");
        })
        .catch((error) => {
            console.error("Error storing conversation: ", error);
        });
        
        } catch (error) {
        console.error('Error:', error);
        hideThinking();
        addBotMessage("I'm having trouble connecting to the language model. Please check your API settings.");
        }
    }
    
    // Generate a random session ID to group conversations
    function generateSessionId() {
        return Math.random().toString(36).substring(2, 15) + Math.random().toString(36).substring(2, 15);
    }
    </script>
</body>
</html>